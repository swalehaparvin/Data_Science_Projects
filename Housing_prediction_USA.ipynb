{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJDHrcV9WLazAfgTaIRsCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/kaggle_projects/blob/main/Housing_prediction_USA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJJZXEIPTeBQ",
        "outputId": "7482ea71-7300-4465-deea-27a0a0e39106"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HtzcxobHSy5z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq1pB9kaStvS",
        "outputId": "f8f77fba-5c5b-44f4-8d61-bb8d96d17b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Combining datasets for preprocessing...\n",
            "Combined dataset shape: (2919, 79)\n",
            "Number of skewed numeric features: 27\n",
            "\n",
            "Handling missing values...\n",
            "              Missing Count    Percent\n",
            "PoolQC                 2909  99.657417\n",
            "MiscFeature            2814  96.402878\n",
            "Alley                  2721  93.216855\n",
            "Fence                  2348  80.438506\n",
            "MasVnrType             1766  60.500171\n",
            "FireplaceQu            1420  48.646797\n",
            "LotFrontage             486  16.649538\n",
            "GarageCond              159   5.447071\n",
            "GarageFinish            159   5.447071\n",
            "GarageYrBlt             159   5.447071\n",
            "GarageQual              159   5.447071\n",
            "GarageType              157   5.378554\n",
            "BsmtExposure             82   2.809181\n",
            "BsmtCond                 82   2.809181\n",
            "BsmtQual                 81   2.774923\n",
            "BsmtFinType2             80   2.740665\n",
            "BsmtFinType1             79   2.706406\n",
            "MasVnrArea               23   0.787941\n",
            "MSZoning                  4   0.137033\n",
            "BsmtHalfBath              2   0.068517\n",
            "All missing values have been handled.\n",
            "\n",
            "Encoding categorical variables...\n",
            "\n",
            "Creating new features...\n",
            "\n",
            "Scaling numeric features...\n",
            "\n",
            "Splitting back into train and test sets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8270857b3021>:77: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  all_data[col].fillna('None', inplace=True)\n",
            "<ipython-input-8-8270857b3021>:86: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  all_data[col].fillna(0, inplace=True)\n",
            "<ipython-input-8-8270857b3021>:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing complete. Files saved.\n",
            "Processed train shape: (1460, 86)\n",
            "Processed test shape: (1459, 86)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Create output directory for plots\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading datasets...\")\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Save the ID column for later\n",
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "\n",
        "# Remove the ID column\n",
        "train.drop('Id', axis=1, inplace=True)\n",
        "test.drop('Id', axis=1, inplace=True)\n",
        "\n",
        "# Save the target variable\n",
        "y_train = train['SalePrice']\n",
        "train.drop('SalePrice', axis=1, inplace=True)\n",
        "\n",
        "# Combine train and test for preprocessing\n",
        "print(\"Combining datasets for preprocessing...\")\n",
        "all_data = pd.concat([train, test], axis=0)\n",
        "print(f\"Combined dataset shape: {all_data.shape}\")\n",
        "\n",
        "# Check for skewness in the target variable\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train, kde=True)\n",
        "plt.title('SalePrice Distribution')\n",
        "plt.savefig('plots/saleprice_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Log transform the target for better model performance\n",
        "y_train_log = np.log1p(y_train)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train_log, kde=True)\n",
        "plt.title('Log(SalePrice+1) Distribution')\n",
        "plt.savefig('plots/log_saleprice_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Analyze numeric features for skewness\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
        "high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n",
        "print(f\"Number of skewed numeric features: {len(high_skew)}\")\n",
        "\n",
        "# Apply Box-Cox transformation to skewed features\n",
        "for feat in high_skew.index:\n",
        "    all_data[feat] = np.log1p(all_data[feat])\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\nHandling missing values...\")\n",
        "missing_data = all_data.isnull().sum().sort_values(ascending=False)\n",
        "missing_data = missing_data[missing_data > 0]\n",
        "missing_percent = (missing_data / len(all_data)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Percent': missing_percent})\n",
        "print(missing_df.head(20))\n",
        "\n",
        "# Features with very high missing values (likely NA means Not Available)\n",
        "na_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
        "           'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
        "           'MasVnrType']\n",
        "\n",
        "# Fill NA with 'None' for categorical features that likely mean Not Available\n",
        "for col in na_cols:\n",
        "    if col in all_data.columns:\n",
        "        all_data[col].fillna('None', inplace=True)\n",
        "\n",
        "# Fill 0 for numeric features that likely mean Not Available\n",
        "zero_cols = ['GarageYrBlt', 'GarageArea', 'GarageCars',\n",
        "             'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n",
        "             'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
        "\n",
        "for col in zero_cols:\n",
        "    if col in all_data.columns:\n",
        "        all_data[col].fillna(0, inplace=True)\n",
        "\n",
        "# For LotFrontage, impute using the median of the neighborhood\n",
        "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
        "    lambda x: x.fillna(x.median()))\n",
        "\n",
        "# For remaining missing values, use mode for categorical and median for numeric\n",
        "cat_cols = all_data.select_dtypes(include=['object']).columns\n",
        "num_cols = all_data.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "for col in cat_cols:\n",
        "    if all_data[col].isnull().sum() > 0:\n",
        "        all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n",
        "\n",
        "for col in num_cols:\n",
        "    if all_data[col].isnull().sum() > 0:\n",
        "        all_data[col].fillna(all_data[col].median(), inplace=True)\n",
        "\n",
        "# Verify no missing values remain\n",
        "assert all_data.isnull().sum().sum() == 0, \"There are still missing values in the dataset\"\n",
        "print(\"All missing values have been handled.\")\n",
        "\n",
        "# Convert categorical variables to numeric using Label Encoding\n",
        "print(\"\\nEncoding categorical variables...\")\n",
        "label_encoder = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    all_data[col] = label_encoder.fit_transform(all_data[col])\n",
        "\n",
        "# Create some new features\n",
        "print(\"\\nCreating new features...\")\n",
        "\n",
        "# Total square footage\n",
        "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
        "\n",
        "# Total bathrooms\n",
        "all_data['TotalBathrooms'] = all_data['FullBath'] + (0.5 * all_data['HalfBath']) + \\\n",
        "                            all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])\n",
        "\n",
        "# House age and when it was remodeled\n",
        "all_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']\n",
        "all_data['Remodeled'] = (all_data['YearRemodAdd'] != all_data['YearBuilt']).astype(int)\n",
        "all_data['RemodAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
        "\n",
        "# Overall quality squared (to emphasize its importance)\n",
        "all_data['OverallQual2'] = all_data['OverallQual'] ** 2\n",
        "\n",
        "# Neighborhood and condition interaction\n",
        "all_data['NeighborhoodQual'] = all_data['Neighborhood'] * all_data['OverallQual']\n",
        "\n",
        "# Scale the numeric features\n",
        "print(\"\\nScaling numeric features...\")\n",
        "scaler = StandardScaler()\n",
        "all_data[num_cols] = scaler.fit_transform(all_data[num_cols])\n",
        "\n",
        "# Split back into train and test\n",
        "print(\"\\nSplitting back into train and test sets...\")\n",
        "train_processed = all_data.iloc[:len(train)]\n",
        "test_processed = all_data.iloc[len(train):]\n",
        "\n",
        "# Save processed data\n",
        "train_processed.to_csv('train_processed.csv', index=False)\n",
        "test_processed.to_csv('test_processed.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False)\n",
        "y_train_log.to_csv('y_train_log.csv', index=False)\n",
        "\n",
        "print(\"\\nPreprocessing complete. Files saved.\")\n",
        "print(f\"Processed train shape: {train_processed.shape}\")\n",
        "print(f\"Processed test shape: {test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "train = pd.read_csv('train_processed.csv')\n",
        "test = pd.read_csv('test_processed.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_train_log = pd.read_csv('y_train_log.csv').values.ravel()\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Target shape: {y_train.shape}\")\n",
        "\n",
        "# Define evaluation metric - RMSE on log scale\n",
        "def rmse_cv(model, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
        "    return rmse\n",
        "\n",
        "# Create a directory for model results\n",
        "os.makedirs('model_results', exist_ok=True)\n",
        "\n",
        "# Initialize models (excluding LightGBM due to system dependency issues)\n",
        "models = {\n",
        "    'Lasso': make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=42)),\n",
        "    'ElasticNet': make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42)),\n",
        "    'Ridge': Ridge(alpha=10.0),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05, max_depth=4, max_features='sqrt', random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "results = {}\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(train, y_train_log)\n",
        "\n",
        "    # Cross-validation score\n",
        "    score = rmse_cv(model, train, y_train_log)\n",
        "    print(f\"{name} RMSE CV: {score.mean():.4f} ({score.std():.4f})\")\n",
        "\n",
        "    # Make predictions\n",
        "    pred = model.predict(train)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_log, pred))\n",
        "    print(f\"{name} Train RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'cv_score': score.mean(),\n",
        "        'cv_std': score.std(),\n",
        "        'train_rmse': train_rmse\n",
        "    }\n",
        "\n",
        "    # Make predictions on test set\n",
        "    test_pred = model.predict(test)\n",
        "    predictions[name] = np.expm1(test_pred)  # Transform back from log scale\n",
        "\n",
        "    # Save model predictions\n",
        "    pd.DataFrame({\n",
        "        'Id': range(1461, 1461 + len(test)),\n",
        "        'SalePrice': np.expm1(test_pred)\n",
        "    }).to_csv(f'model_results/{name}_predictions.csv', index=False)\n",
        "\n",
        "# Visualize model performance\n",
        "plt.figure(figsize=(12, 6))\n",
        "cv_scores = [results[name]['cv_score'] for name in models.keys()]\n",
        "train_scores = [results[name]['train_rmse'] for name in models.keys()]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, cv_scores, width, label='CV RMSE')\n",
        "plt.bar(x + width/2, train_scores, width, label='Train RMSE')\n",
        "plt.xticks(x, models.keys(), rotation=45)\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_results/model_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "# Create ensemble prediction (weighted average of top models)\n",
        "print(\"\\nCreating ensemble prediction...\")\n",
        "ensemble_weights = {\n",
        "    'XGBoost': 0.5,\n",
        "    'GradientBoosting': 0.3,\n",
        "    'Ridge': 0.2\n",
        "}\n",
        "\n",
        "ensemble_pred = np.zeros(len(test))\n",
        "for name, weight in ensemble_weights.items():\n",
        "    ensemble_pred += weight * predictions[name]\n",
        "\n",
        "# Save ensemble prediction\n",
        "pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': ensemble_pred\n",
        "}).to_csv('model_results/ensemble_predictions.csv', index=False)\n",
        "\n",
        "# Format final submission\n",
        "print(\"\\nPreparing final submission file...\")\n",
        "submission = pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': ensemble_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nModel training and prediction complete.\")\n",
        "print(\"Final submission file created: submission.csv\")\n",
        "\n",
        "# Save model performance summary\n",
        "with open('model_results/model_summary.txt', 'w') as f:\n",
        "    f.write(\"Model Performance Summary\\n\")\n",
        "    f.write(\"=======================\\n\\n\")\n",
        "    for name in models.keys():\n",
        "        f.write(f\"{name}:\\n\")\n",
        "        f.write(f\"  CV RMSE: {results[name]['cv_score']:.4f} (±{results[name]['cv_std']:.4f})\\n\")\n",
        "        f.write(f\"  Train RMSE: {results[name]['train_rmse']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"\\nEnsemble Model Weights:\\n\")\n",
        "    for name, weight in ensemble_weights.items():\n",
        "        f.write(f\"  {name}: {weight}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrmr11gVUILW",
        "outputId": "6aec2957-28be-4548-c02c-ff7424887f85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n",
            "Train shape: (1460, 86)\n",
            "Test shape: (1459, 86)\n",
            "Target shape: (1460,)\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Training Lasso...\n",
            "Lasso RMSE CV: 0.1397 (0.0232)\n",
            "Lasso Train RMSE: 0.1238\n",
            "\n",
            "Training ElasticNet...\n",
            "ElasticNet RMSE CV: 0.1399 (0.0231)\n",
            "ElasticNet Train RMSE: 0.1236\n",
            "\n",
            "Training Ridge...\n",
            "Ridge RMSE CV: 0.1395 (0.0232)\n",
            "Ridge Train RMSE: 0.1233\n",
            "\n",
            "Training RandomForest...\n",
            "RandomForest RMSE CV: 0.1447 (0.0182)\n",
            "RandomForest Train RMSE: 0.0539\n",
            "\n",
            "Training GradientBoosting...\n",
            "GradientBoosting RMSE CV: 0.1247 (0.0166)\n",
            "GradientBoosting Train RMSE: 0.0160\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost RMSE CV: 0.1265 (0.0164)\n",
            "XGBoost Train RMSE: 0.0180\n",
            "\n",
            "Creating ensemble prediction...\n",
            "\n",
            "Preparing final submission file...\n",
            "\n",
            "Model training and prediction complete.\n",
            "Final submission file created: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('model_results', exist_ok=True)\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "train = pd.read_csv('train_processed.csv')\n",
        "test = pd.read_csv('test_processed.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_train_log = pd.read_csv('y_train_log.csv').values.ravel()\n",
        "\n",
        "print(f\"Original train shape: {train.shape}\")\n",
        "print(f\"Original test shape: {test.shape}\")\n",
        "\n",
        "# Reduce dimensionality using feature selection\n",
        "print(\"\\nReducing dimensionality...\")\n",
        "k_best_features = 20  # Select top 20 features\n",
        "selector = SelectKBest(f_regression, k=k_best_features)\n",
        "train_reduced = selector.fit_transform(train, y_train_log)\n",
        "test_reduced = selector.transform(test)\n",
        "\n",
        "print(f\"Reduced train shape: {train_reduced.shape}\")\n",
        "print(f\"Reduced test shape: {test_reduced.shape}\")\n",
        "\n",
        "# Train a simple Ridge model\n",
        "print(\"\\nTraining Ridge model...\")\n",
        "model = Ridge(alpha=10.0, random_state=42)\n",
        "model.fit(train_reduced, y_train_log)\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\nMaking predictions...\")\n",
        "test_pred_log = model.predict(test_reduced)\n",
        "test_pred = np.expm1(test_pred_log)  # Transform back from log scale\n",
        "\n",
        "# Format final submission\n",
        "print(\"\\nPreparing final submission file...\")\n",
        "submission = pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nModel training and prediction complete.\")\n",
        "print(\"Final submission file created: submission.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oalBIwl4URRu",
        "outputId": "6112f9ff-49c6-49e7-c6e2-7c036424d5a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n",
            "Original train shape: (1460, 86)\n",
            "Original test shape: (1459, 86)\n",
            "\n",
            "Reducing dimensionality...\n",
            "Reduced train shape: (1460, 20)\n",
            "Reduced test shape: (1459, 20)\n",
            "\n",
            "Training Ridge model...\n",
            "\n",
            "Making predictions...\n",
            "\n",
            "Preparing final submission file...\n",
            "\n",
            "Model training and prediction complete.\n",
            "Final submission file created: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "train = pd.read_csv('train_processed.csv')\n",
        "test = pd.read_csv('test_processed.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_train_log = pd.read_csv('y_train_log.csv').values.ravel()\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Target shape: {y_train.shape}\")\n",
        "\n",
        "# Define evaluation metric - RMSE on log scale\n",
        "def rmse_cv(model, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
        "    return rmse\n",
        "\n",
        "# Create a directory for model results\n",
        "os.makedirs('model_results', exist_ok=True)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Lasso': make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=42)),\n",
        "    'ElasticNet': make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42)),\n",
        "    'Ridge': Ridge(alpha=10.0),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05, max_depth=4, max_features='sqrt', random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMRegressor(\n",
        "        objective='regression',\n",
        "        num_leaves=31,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "results = {}\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(train, y_train_log)\n",
        "\n",
        "    # Cross-validation score\n",
        "    score = rmse_cv(model, train, y_train_log)\n",
        "    print(f\"{name} RMSE CV: {score.mean():.4f} ({score.std():.4f})\")\n",
        "\n",
        "    # Make predictions\n",
        "    pred = model.predict(train)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_log, pred))\n",
        "    print(f\"{name} Train RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'cv_score': score.mean(),\n",
        "        'cv_std': score.std(),\n",
        "        'train_rmse': train_rmse\n",
        "    }\n",
        "\n",
        "    # Make predictions on test set\n",
        "    test_pred = model.predict(test)\n",
        "    predictions[name] = np.expm1(test_pred)  # Transform back from log scale\n",
        "\n",
        "    # Save model predictions\n",
        "    pd.DataFrame({\n",
        "        'Id': range(1461, 1461 + len(test)),\n",
        "        'SalePrice': np.expm1(test_pred)\n",
        "    }).to_csv(f'model_results/{name}_predictions.csv', index=False)\n",
        "\n",
        "# Visualize model performance\n",
        "plt.figure(figsize=(12, 6))\n",
        "cv_scores = [results[name]['cv_score'] for name in models.keys()]\n",
        "train_scores = [results[name]['train_rmse'] for name in models.keys()]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, cv_scores, width, label='CV RMSE')\n",
        "plt.bar(x + width/2, train_scores, width, label='Train RMSE')\n",
        "plt.xticks(x, models.keys(), rotation=45)\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_results/model_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "# Create ensemble prediction (weighted average of top models)\n",
        "print(\"\\nCreating ensemble prediction...\")\n",
        "ensemble_weights = {\n",
        "    'XGBoost': 0.4,\n",
        "    'LightGBM': 0.3,\n",
        "    'GradientBoosting': 0.2,\n",
        "    'Ridge': 0.1\n",
        "}\n",
        "\n",
        "ensemble_pred = np.zeros(len(test))\n",
        "for name, weight in ensemble_weights.items():\n",
        "    ensemble_pred += weight * predictions[name]\n",
        "\n",
        "# Save ensemble prediction\n",
        "pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': ensemble_pred\n",
        "}).to_csv('model_results/ensemble_predictions.csv', index=False)\n",
        "\n",
        "# Format final submission\n",
        "print(\"\\nPreparing final submission file...\")\n",
        "submission = pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': ensemble_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nModel training and prediction complete.\")\n",
        "print(\"Final submission file created: submission.csv\")\n",
        "\n",
        "# Save model performance summary\n",
        "with open('model_results/model_summary.txt', 'w') as f:\n",
        "    f.write(\"Model Performance Summary\\n\")\n",
        "    f.write(\"=======================\\n\\n\")\n",
        "    for name in models.keys():\n",
        "        f.write(f\"{name}:\\n\")\n",
        "        f.write(f\"  CV RMSE: {results[name]['cv_score']:.4f} (±{results[name]['cv_std']:.4f})\\n\")\n",
        "        f.write(f\"  Train RMSE: {results[name]['train_rmse']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"\\nEnsemble Model Weights:\\n\")\n",
        "    for name, weight in ensemble_weights.items():\n",
        "        f.write(f\"  {name}: {weight}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-487tyOJUVnf",
        "outputId": "5b9212b4-6397-4f25-864c-c5936b47a895"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n",
            "Train shape: (1460, 86)\n",
            "Test shape: (1459, 86)\n",
            "Target shape: (1460,)\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Training Lasso...\n",
            "Lasso RMSE CV: 0.1397 (0.0232)\n",
            "Lasso Train RMSE: 0.1238\n",
            "\n",
            "Training ElasticNet...\n",
            "ElasticNet RMSE CV: 0.1399 (0.0231)\n",
            "ElasticNet Train RMSE: 0.1236\n",
            "\n",
            "Training Ridge...\n",
            "Ridge RMSE CV: 0.1395 (0.0232)\n",
            "Ridge Train RMSE: 0.1233\n",
            "\n",
            "Training RandomForest...\n",
            "RandomForest RMSE CV: 0.1447 (0.0182)\n",
            "RandomForest Train RMSE: 0.0539\n",
            "\n",
            "Training GradientBoosting...\n",
            "GradientBoosting RMSE CV: 0.1247 (0.0166)\n",
            "GradientBoosting Train RMSE: 0.0160\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost RMSE CV: 0.1265 (0.0164)\n",
            "XGBoost Train RMSE: 0.0180\n",
            "\n",
            "Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4077\n",
            "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 80\n",
            "[LightGBM] [Info] Start training from score 12.024057\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3815\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n",
            "[LightGBM] [Info] Start training from score 12.030658\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3808\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n",
            "[LightGBM] [Info] Start training from score 12.016898\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3791\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n",
            "[LightGBM] [Info] Start training from score 12.022759\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3812\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n",
            "[LightGBM] [Info] Start training from score 12.027933\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3812\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n",
            "[LightGBM] [Info] Start training from score 12.022040\n",
            "LightGBM RMSE CV: 0.1358 (0.0168)\n",
            "LightGBM Train RMSE: 0.0052\n",
            "\n",
            "Creating ensemble prediction...\n",
            "\n",
            "Preparing final submission file...\n",
            "\n",
            "Model training and prediction complete.\n",
            "Final submission file created: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('model_results', exist_ok=True)\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "train = pd.read_csv('train_processed.csv')\n",
        "test = pd.read_csv('test_processed.csv')\n",
        "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
        "y_train_log = pd.read_csv('y_train_log.csv').values.ravel()\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Target shape: {y_train.shape}\")\n",
        "\n",
        "# Define evaluation metric - RMSE on log scale\n",
        "def rmse_cv(model, X, y):\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
        "    return rmse\n",
        "\n",
        "# Initialize lightweight models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=10.0, random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "print(\"\\nTraining and evaluating models...\")\n",
        "results = {}\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(train, y_train_log)\n",
        "\n",
        "    # Cross-validation score\n",
        "    score = rmse_cv(model, train, y_train_log)\n",
        "    print(f\"{name} RMSE CV: {score.mean():.4f} ({score.std():.4f})\")\n",
        "\n",
        "    # Make predictions\n",
        "    pred = model.predict(train)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_log, pred))\n",
        "    print(f\"{name} Train RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'cv_score': score.mean(),\n",
        "        'cv_std': score.std(),\n",
        "        'train_rmse': train_rmse\n",
        "    }\n",
        "\n",
        "    # Make predictions on test set\n",
        "    test_pred = model.predict(test)\n",
        "    predictions[name] = np.expm1(test_pred)  # Transform back from log scale\n",
        "\n",
        "    # Save model predictions\n",
        "    pd.DataFrame({\n",
        "        'Id': range(1461, 1461 + len(test)),\n",
        "        'SalePrice': np.expm1(test_pred)\n",
        "    }).to_csv(f'model_results/{name}_predictions.csv', index=False)\n",
        "\n",
        "# Use Ridge as the final model (typically more robust than simple linear regression)\n",
        "print(\"\\nPreparing final submission file...\")\n",
        "submission = pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(test)),\n",
        "    'SalePrice': predictions['Ridge']\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nModel training and prediction complete.\")\n",
        "print(\"Final submission file created: submission.csv\")\n",
        "\n",
        "# Save model performance summary\n",
        "with open('model_results/model_summary.txt', 'w') as f:\n",
        "    f.write(\"Model Performance Summary\\n\")\n",
        "    f.write(\"=======================\\n\\n\")\n",
        "    for name in models.keys():\n",
        "        f.write(f\"{name}:\\n\")\n",
        "        f.write(f\"  CV RMSE: {results[name]['cv_score']:.4f} (±{results[name]['cv_std']:.4f})\\n\")\n",
        "        f.write(f\"  Train RMSE: {results[name]['train_rmse']:.4f}\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r26OIELdUdiB",
        "outputId": "67ac341c-857b-46b8-a733-96edf69a61fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed data...\n",
            "Train shape: (1460, 86)\n",
            "Test shape: (1459, 86)\n",
            "Target shape: (1460,)\n",
            "\n",
            "Training and evaluating models...\n",
            "\n",
            "Training LinearRegression...\n",
            "LinearRegression RMSE CV: 0.1432 (0.0230)\n",
            "LinearRegression Train RMSE: 0.1225\n",
            "\n",
            "Training Ridge...\n",
            "Ridge RMSE CV: 0.1395 (0.0232)\n",
            "Ridge Train RMSE: 0.1233\n",
            "\n",
            "Preparing final submission file...\n",
            "\n",
            "Model training and prediction complete.\n",
            "Final submission file created: submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Prediction"
      ],
      "metadata": {
        "id": "wDO-8dMBUvsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('model_results', exist_ok=True)\n",
        "\n",
        "# Load the original data (not the processed data that's causing issues)\n",
        "print(\"Loading original data...\")\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "\n",
        "# Extract the target variable\n",
        "y_train = train['SalePrice']\n",
        "print(f\"Target mean: {y_train.mean()}\")\n",
        "print(f\"Target median: {y_train.median()}\")\n",
        "\n",
        "# Identify key numeric features that are likely to influence house prices\n",
        "key_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "print(f\"Selected key features: {key_features}\")\n",
        "\n",
        "# Create a simple prediction based on OverallQual (overall quality rating)\n",
        "# This is the most important feature for house prices\n",
        "print(\"\\nCreating predictions based on OverallQual groups...\")\n",
        "\n",
        "# Group by OverallQual and calculate mean SalePrice for each group\n",
        "qual_price_map = train.groupby('OverallQual')['SalePrice'].median().to_dict()\n",
        "\n",
        "# For any missing OverallQual in test, use the median of all prices\n",
        "median_price = y_train.median()\n",
        "\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "for qual in test['OverallQual']:\n",
        "    if qual in qual_price_map:\n",
        "        predictions.append(qual_price_map[qual])\n",
        "    else:\n",
        "        predictions.append(median_price)\n",
        "\n",
        "# Format final submission\n",
        "print(\"\\nPreparing final submission file...\")\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test['Id'],\n",
        "    'SalePrice': predictions\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nPrediction complete.\")\n",
        "print(\"Final submission file created: submission.csv\")\n",
        "print(f\"Predictions mean: {np.mean(predictions)}\")\n",
        "print(f\"Predictions median: {np.median(predictions)}\")\n",
        "\n",
        "# Save a summary of the approach\n",
        "with open('model_results/approach_summary.txt', 'w') as f:\n",
        "    f.write(\"House Price Prediction Approach Summary\\n\")\n",
        "    f.write(\"=====================================\\n\\n\")\n",
        "    f.write(\"Due to system constraints, a simplified statistical approach was used:\\n\\n\")\n",
        "    f.write(\"1. The 'OverallQual' feature was identified as the most important predictor of house prices\\n\")\n",
        "    f.write(\"2. For each 'OverallQual' value in the test set, the median sale price of houses with that quality in the training set was used\\n\")\n",
        "    f.write(\"3. For any 'OverallQual' values not present in training, the overall median price was used\\n\\n\")\n",
        "    f.write(\"This approach provides a reasonable baseline prediction without requiring complex model training.\\n\\n\")\n",
        "    f.write(\"Quality-Price Mapping:\\n\")\n",
        "    for qual, price in qual_price_map.items():\n",
        "        f.write(f\"  Quality {qual}: ${price:.2f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZxSe1J3UuhD",
        "outputId": "89c494d5-609d-4381-d051-4d42a55dfd18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading original data...\n",
            "Train shape: (1460, 81)\n",
            "Test shape: (1459, 80)\n",
            "Target mean: 180921.19589041095\n",
            "Target median: 163000.0\n",
            "Selected key features: ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
            "\n",
            "Creating predictions based on OverallQual groups...\n",
            "\n",
            "Preparing final submission file...\n",
            "\n",
            "Prediction complete.\n",
            "Final submission file created: submission.csv\n",
            "Predictions mean: 177674.08567511995\n",
            "Predictions median: 160000.0\n"
          ]
        }
      ]
    }
  ]
}