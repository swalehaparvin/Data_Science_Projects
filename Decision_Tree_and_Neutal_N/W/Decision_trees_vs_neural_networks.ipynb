{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4QIPYAoplP3YuQIUOIhYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/kaggle_projects/blob/main/Decision_trees_vs_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a decision tree classifier to classify income levels based on multiple features including age, education level, and hours worked per week, and extract the learned rules that explain the decision. Then, compare its performance with an MLPClassifier trained on the same data.\n",
        "\n",
        "X_train, X_test, y_train, and y_test are pre-loaded for you. The accuracy_score and export_text functions are also imported for you.\n",
        "\n",
        "Train the MLPClassifier model.\n",
        "Derive the predictions on the test set.\n",
        "Compute the model's test accuracy."
      ],
      "metadata": {
        "id": "FkdV0F1jtgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,           # Number of samples\n",
        "    n_features=6,             # Number of features\n",
        "    n_informative=4,          # Number of informative features\n",
        "    n_redundant=1,            # Number of redundant features\n",
        "    n_clusters_per_class=1,   # Number of clusters per class\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "feature_names = ['age', 'income', 'education_years', 'experience', 'hours_worked', 'location_score']\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Add some realistic scaling to make features more interpretable\n",
        "X['age'] = (X['age'] * 10 + 35).astype(int)  # Age between 25-45\n",
        "X['income'] = (X['income'] * 20000 + 50000).astype(int)  # Income between 30k-70k\n",
        "X['education_years'] = (X['education_years'] * 5 + 16).astype(int)  # Education 11-21 years\n",
        "X['experience'] = (X['experience'] * 8 + 5).astype(int)  # Experience 0-15 years\n",
        "X['hours_worked'] = (X['hours_worked'] * 15 + 40).astype(int)  # Hours 25-55\n",
        "X['location_score'] = (X['location_score'] * 50 + 50).astype(int)  # Location score 0-100\n",
        "\n",
        "print(\"Sample of the dataset:\")\n",
        "print(X.head())\n",
        "print(f\"\\nTarget distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 20% for testing\n",
        "    random_state=42,  # For reproducible results\n",
        "    stratify=y        # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "# Decision Tree Classifier\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DECISION TREE CLASSIFIER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Extract the rules\n",
        "rules = export_text(model, feature_names=list(X_train.columns))\n",
        "print(\"Decision Tree Rules:\")\n",
        "print(rules)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Neural Network (MLP) Classifier\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MULTI-LAYER PERCEPTRON CLASSIFIER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model_mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(36, 12),\n",
        "    random_state=42,\n",
        "    max_iter=1000  # Increase max iterations to avoid convergence warnings\n",
        ")\n",
        "\n",
        "# Train the MLPClassifier\n",
        "model_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Derive the predictions on the test set\n",
        "y_pred_mlp = model_mlp.predict(X_test)\n",
        "\n",
        "# Compute the test accuracy\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "print(f\"MLP Accuracy: {accuracy_mlp:.3f}\")\n",
        "\n",
        "# Compare both models\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.3f}\")\n",
        "print(f\"MLP Classifier Accuracy: {accuracy_mlp:.3f}\")\n",
        "\n",
        "if accuracy > accuracy_mlp:\n",
        "    print(\"Decision Tree performed better!\")\n",
        "elif accuracy_mlp > accuracy:\n",
        "    print(\"MLP Classifier performed better!\")\n",
        "else:\n",
        "    print(\"Both models performed equally well!\")\n",
        "\n",
        "# Feature importance for Decision Tree\n",
        "print(\"\\nFeature Importance (Decision Tree):\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9bZpJfWXK2T",
        "outputId": "7354def8-86e6-43c1-cf3c-267fabd161ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample dataset...\n",
            "Sample of the dataset:\n",
            "   age  income  education_years  experience  hours_worked  location_score\n",
            "0   32   36242                9          -4            54              67\n",
            "1   31   79839                8           7            48              47\n",
            "2   43   20735                1           2            62               4\n",
            "3   28   27261                8          -4            26              96\n",
            "4   17   48887               11           0           -10              69\n",
            "\n",
            "Target distribution: [502 498]\n",
            "\n",
            "Training set size: (800, 6)\n",
            "Testing set size: (200, 6)\n",
            "\n",
            "==================================================\n",
            "DECISION TREE CLASSIFIER\n",
            "==================================================\n",
            "Decision Tree Rules:\n",
            "|--- hours_worked <= 42.50\n",
            "|   |--- income <= 71944.00\n",
            "|   |   |--- hours_worked <= 38.50\n",
            "|   |   |   |--- class: 0\n",
            "|   |   |--- hours_worked >  38.50\n",
            "|   |   |   |--- class: 0\n",
            "|   |--- income >  71944.00\n",
            "|   |   |--- hours_worked <= 2.50\n",
            "|   |   |   |--- class: 0\n",
            "|   |   |--- hours_worked >  2.50\n",
            "|   |   |   |--- class: 1\n",
            "|--- hours_worked >  42.50\n",
            "|   |--- income <= 22432.50\n",
            "|   |   |--- education_years <= 9.50\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- education_years >  9.50\n",
            "|   |   |   |--- class: 0\n",
            "|   |--- income >  22432.50\n",
            "|   |   |--- income <= 25300.50\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- income >  25300.50\n",
            "|   |   |   |--- class: 1\n",
            "\n",
            "Decision Tree Accuracy: 0.915\n",
            "\n",
            "==================================================\n",
            "MULTI-LAYER PERCEPTRON CLASSIFIER\n",
            "==================================================\n",
            "MLP Accuracy: 0.535\n",
            "\n",
            "==================================================\n",
            "MODEL COMPARISON\n",
            "==================================================\n",
            "Decision Tree Accuracy: 0.915\n",
            "MLP Classifier Accuracy: 0.535\n",
            "Decision Tree performed better!\n",
            "\n",
            "Feature Importance (Decision Tree):\n",
            "           feature  importance\n",
            "4     hours_worked    0.743006\n",
            "2  education_years    0.150848\n",
            "1           income    0.106146\n",
            "0              age    0.000000\n",
            "3       experience    0.000000\n",
            "5   location_score    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d65b683"
      },
      "source": [
        "**Note:** The following cell contains placeholder code for loading and splitting data. Please replace it with your actual data loading and splitting logic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44622b90"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Placeholder code: Replace this with your actual data loading\n",
        "# For example, if you have a CSV file named 'income_data.csv':\n",
        "# df = pd.read_csv('income_data.csv')\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with features and a target variable\n",
        "# X = df.drop('income_level', axis=1) # Replace 'income_level' with your target column name\n",
        "# y = df['income_level'] # Replace 'income_level' with your target column name\n",
        "\n",
        "# Placeholder code: Replace this with your actual data splitting\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Example placeholder data (replace with your actual data)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ca30b1",
        "outputId": "6ac71f94-9808-461b-de0e-5edf20c8fa26"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier # Import MLPClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42, max_depth=2)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Extract the rules\n",
        "rules = export_text(model, feature_names=[f'feature_{i}' for i in range(X_train.shape[1])]) # Use generic feature names\n",
        "print(rules)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(36, 12), random_state=42)\n",
        "# Train the MLPClassifier\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Derive the predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the test accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"MLPClassifier Accuracy: {accuracy:.2f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|--- feature_5 <= -0.35\n",
            "|   |--- feature_14 <= -1.81\n",
            "|   |   |--- class: 1\n",
            "|   |--- feature_14 >  -1.81\n",
            "|   |   |--- class: 0\n",
            "|--- feature_5 >  -0.35\n",
            "|   |--- feature_18 <= -0.19\n",
            "|   |   |--- class: 1\n",
            "|   |--- feature_18 >  -0.19\n",
            "|   |   |--- class: 1\n",
            "\n",
            "Decision Tree Accuracy: 0.86\n",
            "MLPClassifier Accuracy: 0.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}